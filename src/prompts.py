STANDARD_PROMPT =   """
                    Given the factual graph: {} and given the counterfactual example: {}
                    and given the knowledge base about the dataset: {}, fill the dictionary!
                    and provide an explanation about the change in classification for the target node, please evaluate also the influences of neighbors nodes."""

ZERO_SHOT_PROMPT =  """
                    Original graph: {}
                    Counterfactual graph: {}
                    Explain why the classification result changed.
                    """
                    
FEW_SHOT_PROMPT = """
    Given the following example:
    
    Original graph: The node 0 has as attributes the words formal, keyword, creat, unit, frequent, stage, enterpris, center, explain, divers, notat, book, attitud, prolifer, fuse, happen, colour, 05, savvysearch, transmiss, everywher, destin, 35, multilay, immens, forag, 500, prompt, grate, dab, emphasis, raster, jinni, cancer, sap and is connected with the nodes 2, 3, 4, 5 and it is classified as Information Retrieval, it is not the target node. The node 1 has as attributes the words applic, present, feedback, joint, media, iter, equip, book, fuse, vertic, firstli, preprocess, tra, multilay, chemic, emphasis, skeptic, imperfect, sap, polar and is connected with the nodes 2, 5 and it is classified as Information Retrieval, it is not the target node. The node 2 has as attributes the words present, time, given, start, convent, higher, seri, frequent, pair, come, planner, u, 2d, held, lexic, compens, fuse, disciplin, grain, rm, diagnos, palm, pilot, storytel, km, invis, cantli, sun, inequ, sacrif, win, poster, f30602, grate, berkeley, bbq, sap and is connected with the nodes 0, 1, 5 and it is classified as Information Retrieval, it is not the target node. The node 3 has as attributes the words time, given, formal, increas, electron, made, via, unit, proxi, attract, constitut, wherea, rotat, dlr, acoust, classroom, prepar, colour, floor, narrow, 35, reject, nine, overrid, successor, 37, unforeseen, disciplinari, toronto and is connected with the node 0 and it is classified as Information Retrieval, it is not the target node. The node 4 has as attributes the words answer, valid, long, list, furthermor, static, reduct, p, frequent, disjunct, arbitrari, intrus, competit, visualis, bottom, boolean, institut, clearli, travers, th, dramat, token, realis, cite, maxq, nonmonoton, tfxldf, eigenspac, inc, fourth, vulner, unless, occas, toot and is connected with the nodes 0, 5 and it is classified as Information Retrieval, it is the target node. The node 5 has as attributes the words present, formal, keyword, unit, frequent, stage, center, notat, book, attitud, prolifer, fuse, happen, colour, 05, savvysearch, everywher, destin, 35, immens, forag, 500, nine, unrel, grate, dab, emphasis, sap and is connected with the nodes 0, 1, 2, 4, 6 and it is classified as Information Retrieval, it is not the target node. The node 6 has as attributes the words  and is connected with the node 5 and it is classified as Agents, it is not the target node. 
    
    Counterfactual graph: The node 0 has as attributes the words keyword, creat, stage, enterpris, notat, prolifer, 05, multilay, immens, dab, emphasis, cancer and is connected with the nodes 2, 3, 4, 5 and it is classified as Machine Learning, it is not the target node. The node 1 has as attributes the words present, joint, book, preprocess, multilay, chemic, emphasis, skeptic, polar and is connected with the nodes 2, 5 and it is classified as Machine Learning, it is not the target node. The node 2 has as attributes the words time, start, convent, come, u, held, lexic, diagnos, palm, invis, sun, inequ, sacrif, win, poster, f30602, berkeley and is connected with the nodes 0, 1, 5 and it is classified as Machine Learning, it is not the target node. The node 3 has as attributes the words time, increas, electron, wherea, reject, overrid, successor, toronto and is connected with the node 0 and it is classified as Machine Learning, it is not the target node. The node 4 has as attributes the words answer, list, reduct, bottom, dramat, token, cite, nonmonoton, eigenspac, fourth, vulner, unless and is connected with the nodes 0, 5 and it is classified as Machine Learning, it is the target node. The node 5 has as attributes the words present, keyword, stage, notat, prolifer, 05, immens, dab, emphasis and is connected with the nodes 0, 1, 2, 4, 6 and it is classified as Machine Learning, it is not the target node. The node 6 has as attributes the words  and is connected with the node 5 and it is classified as Machine Learning, it is not the target node. 
    
    Dataset knowledge: The CiteSeer dataset is a citation network composed of scientific publications. It's widely used in machine learning and network analysis, especially for tasks involving graph neural networks (GNNs).Key Features:- Nodes: 3,312 documents (scientific publications).- Edges: 4,732 citation links. Each edge represents a citation from one document to another. The graph is undirected, meaning that if document A cites document B, there is a link between both.- Node Features: Each document is described by a 3,703-dimensional binary feature vector. Each dimension corresponds to a unique word from the dictionary, and the value is 1 if the word appears in the document and 0 otherwise (bag-of-words model).- Classes: Each document belongs to one of six classes:  1. Agents  2. Artificial Intelligence  3. Database  4. Information Retrieval  5. Machine Learning  6. Human-Computer InteractionUsage in Machine Learning:- Node Classification: Predict the category of a document based on its content and citation relationships.- Graph Representation Learning: Learn embeddings that capture both textual content and citation network structure.- Benchmarking Graph Neural Networks: Evaluate GNN architectures like GCN, GAT, and others., fill the dictionary!and provide an explanation about the change in classification for the target node, please evaluate also the influences of neighbors nodes.
    
    Explanation: The target node (Node 4) was originally classified as Information Retrieval. In the counterfactual scenario, it is classified as Machine Learning. The change in classification is primarily due to the removal of several attributes such as "valid," "long," "furthermor," "static," "boolean," "institut," "clearli," "travers," "th," "realis," "maxq," "inc," "occas," and "toot." Additionally, the neighboring nodes (Nodes 0 and 5) also underwent changes in their attributes, which likely influenced the classification of Node 4. Specifically, Node 0 lost attributes like "formal," "creativ," "unit," "explain," "divers," "transmiss," "prompt," "raster," "jinni," "cancer," while Node 5 lost attributes like "formal," "unit," "nine," "unrel," "savvysearch," "destin," "forag," "five," "grate," "nine," "unrel," "savvysearch," "destin," "forag," "five." These changes in the neighborhood context contributed to the shift in Node 4's classification from Information Retrieval to Machine Learning.

    Now for a new graph:
    Original graph: {}
    Counterfactual graph: {}
    Dataset Knwoledge: {}
    Based on the above examples, explain why the target nodeâ€™s classification changed. Include how node features or neighbors contributed to the new prediction.
    """